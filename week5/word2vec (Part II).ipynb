{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v9/gjq5vjbs1qv8bm8yhcn91xb80000gn/T/ipykernel_287/3154583868.py:11: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  ax = sns.distplot(vector, kde=False, rug=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "vector = nlp(u'banana').vector\n",
    "\n",
    "ax = sns.distplot(vector, kde=False, rug=True)\n",
    "t = ax.set_title('Histogram of Feature Values')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Techniques\n",
    "\n",
    "### Subsampling\n",
    "\n",
    "What do we do with highly frequent words like `the` or `of`? We don't gain a ton of meaning from training on these words, and they become computationally expensive since they appear so frequently:\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/subsampling.png \"http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\")\n",
    "In the image above, $z(w_i)$ is the frequency of that particular word divided by the total number of words in the entire corpus. For instance, if a corpus of text has 50 words, and the word `dog` appears 3 times, $z(w_{dog}) = 0.06$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# write subsampling function\n",
    "def subsample(z):\n",
    "    return ((z * 1000) ** 0.5 + 1) * (0.001 / z)\n",
    "\n",
    "# plot this function:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Z = list(np.linspace(0,1,100))\n",
    "probability_of_keeping = list(map( lambda z: subsample(z), Z))\n",
    "\n",
    "plt.scatter(Z, probability_of_keeping)\n",
    "plt.xlabel(\"Frequency word appears in corpus\")\n",
    "plt.ylabel(\"Probability of keeping\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Word Embeddings\n",
    "\n",
    "#### How to handle **Out Of Vocabulary (OOV)** words?\n",
    "Although **word2vec** and **FastText** include a significant vocabulary size, there will inevitably be words that are not included. For instance, if you are analyzing text conversations using word embeddings pretrained on Wikipedia text (which typically has more formal vocabulary than everyday language), how will you account for the following words?\n",
    "\n",
    "- DM\n",
    "- ROFLMAO\n",
    "- bae\n",
    "- 😃\n",
    "- #10YearChallenge\n",
    "- wut\n",
    "\n",
    "#### Potential solution: use word embeddings if they are available, and otherwise initialize the weights to random.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "def vectorize_word(input_word: str, D=50):\n",
    "    \"\"\"\n",
    "    D: an integer that represents the length (dimensionality of the word embeddings)\n",
    "    word_embeddings: A dictionary object with the string word as the key, and the embedding vector of \n",
    "    length D as the values.\n",
    "    For instance, word_embeddings[\"cat\"] will return [2.3, 4.5, 6.1, -2.2, ...]\n",
    "    \"\"\"\n",
    "    if input_word in word_embeddings.keys():\n",
    "        return word_embeddings[input_word]\n",
    "    else:\n",
    "        return np.random.rand(D)\n",
    "```\n",
    "\n",
    "##### Should we update the word embedding matrices during the model training step?\n",
    "- Ideally, you'd only want to be able to update the specific weights that were randomly initialized (since the rest of the weights are by definition pre-trained and are already pretty good). However, most deep learning libraries do not allow you to easily select which specific weight elements to apply backpropagation to- you either update all weights or you update none. In practice, most data scientists will \"freeze\" the word embedding layer:\n",
    "\n",
    "In Keras:\n",
    "```python\n",
    "word_embedding_layer.trainable = False # by default, trainable is set to true in Keras\n",
    "```\n",
    "In Tensorflow:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "N = 300 # number of words\n",
    "D = 50 # of dimensions in embeddings\n",
    "initial_word_embeddings = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "tensor = tf.constant(initial_word_embeddings, shape=[N, D])\n",
    "```\n",
    "\n",
    "- Ambiguity around **Domain-specific words**: using a generic pre-trained word embedding will not capture the semantic meaning of the word **sack** when it is used in the context of American football:\n",
    "![sack](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/football-bag-sack-diff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://radimrehurek.com/gensim/models/word2vec.html\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "path = get_tmpfile(\"word2vec.model\")\n",
    "model = Word2Vec(common_texts, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=14, vector_size=100, alpha=0.025>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-5.3613470e-04,  2.3713129e-04,  5.1058987e-03,  9.0121645e-03,\n",
       "       -9.3048755e-03, -7.1178679e-03,  6.4615863e-03,  8.9742010e-03,\n",
       "       -5.0173230e-03, -3.7642212e-03,  7.3823351e-03, -1.5340967e-03,\n",
       "       -4.5365901e-03,  6.5550972e-03, -4.8604906e-03, -1.8166588e-03,\n",
       "        2.8769472e-03,  9.9250535e-04, -8.2884440e-03, -9.4515411e-03,\n",
       "        7.3134978e-03,  5.0723492e-03,  6.7602862e-03,  7.6203729e-04,\n",
       "        6.3516032e-03, -3.4052967e-03, -9.4666512e-04,  5.7692323e-03,\n",
       "       -7.5236904e-03, -3.9367126e-03, -7.5138286e-03, -9.3064696e-04,\n",
       "        9.5404182e-03, -7.3211812e-03, -2.3341118e-03, -1.9382533e-03,\n",
       "        8.0786739e-03, -5.9320745e-03,  4.4811048e-05, -4.7555189e-03,\n",
       "       -9.6055325e-03,  5.0082025e-03, -8.7615345e-03, -4.3924255e-03,\n",
       "       -3.4532673e-05, -2.9693675e-04, -7.6633105e-03,  9.6167298e-03,\n",
       "        4.9833693e-03,  9.2346733e-03, -8.1605939e-03,  4.4963211e-03,\n",
       "       -4.1372753e-03,  8.2342041e-04,  8.5003115e-03, -4.4622524e-03,\n",
       "        4.5182519e-03, -6.7880899e-03, -3.5491206e-03,  9.4007580e-03,\n",
       "       -1.5777332e-03,  3.2050317e-04, -4.1417391e-03, -7.6838490e-03,\n",
       "       -1.5085860e-03,  2.4710456e-03, -8.8896940e-04,  5.5365730e-03,\n",
       "       -2.7444486e-03,  2.2605830e-03,  5.4565002e-03,  8.3476501e-03,\n",
       "       -1.4531007e-03, -9.2094336e-03,  4.3706587e-03,  5.7202944e-04,\n",
       "        7.4436460e-03, -8.1418752e-04, -2.6398660e-03, -8.7558115e-03,\n",
       "       -8.5749477e-04,  2.8268259e-03,  5.4026032e-03,  7.0541641e-03,\n",
       "       -5.7040025e-03,  1.8595017e-03,  6.0911127e-03, -4.7990368e-03,\n",
       "       -3.1078621e-03,  6.7990827e-03,  1.6321423e-03,  1.8928670e-04,\n",
       "        3.4738265e-03,  2.1778070e-04,  9.6210763e-03,  5.0627030e-03,\n",
       "       -8.9189457e-03, -7.0433435e-03,  9.0186729e-04,  6.3931402e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "             ['this', 'is', 'the', 'second', 'sentence'],\n",
    "             ['yet', 'another', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['and', 'the', 'final', 'sentence'],\n",
    "            [\"first\", \"and\", \"second\", \"sentence\"]]\n",
    "# train model\n",
    "# you can also specify an alpha, which is a hyperparameter learning rate\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary1\n",
    "model.wv.key_to_index\n",
    "model.wv.get_vector(\"sentence\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Your Own Word2Vec Embeddings Using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = open(\"../datasets/good_amazon_toy_reviews.txt\").readlines() + open(\"../datasets/poor_amazon_toy_reviews.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [word_tokenize(review) for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(docs, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Amazon', 0.8515133261680603),\n",
       " ('sale', 0.7785695195198059),\n",
       " ('here', 0.6191052794456482),\n",
       " ('Amazon.com', 0.5998008847236633),\n",
       " ('Thursday', 0.5991731882095337),\n",
       " ('ebay', 0.5956100225448608),\n",
       " ('whim', 0.5914636850357056),\n",
       " ('Friday', 0.5784347057342529),\n",
       " ('market', 0.5771557688713074),\n",
       " ('eBay', 0.5761690735816956)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"amazon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GoogleNews word2vec vectors\n",
    "\n",
    "You can download the entire dataset for word2vec trained via negative sampling [here](https://code.google.com/archive/p/word2vec/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING: RUNNING THIS CELL WILL DOWNLOAD A 1GB dataset to your computer\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "vec_king = wv['homework']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the entire Google News word embedding vectors\n",
    "from gensim.models import KeyedVectors\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "\n",
    "# word analogies\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most similar words for a target word\n",
    "model.most_similar(\"cappucino\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major problem with `word2vec` and other traditional word embedding strategies is how to deal with out of bag (OOB) or out of vocabulary (OOV) words.\n",
    "\n",
    "The vector embedding for the word `photosynthesis` will be the average of `pho`, `phot`, `photo`, etc.\n",
    "\n",
    "This allow FastText to embed many words that are not traditionally in the trained vocabulary (`photogenic` may be OOB, but `photo` will be available).\n",
    "\n",
    "### When to use?\n",
    "\n",
    "- traditionally, each individual word is trained onto a new word embedding\n",
    "- in many languages (including English), many words are morphologically derivative from each other. \n",
    "- use case when your corpus contains high-value, morphologically diverse, rare words (`photosynthesis`, `transcendentalism`)\n",
    "- may also be effective when your text contains lots of misspellings or abbreviations (ie. SMS, digital conversations)\n",
    "\n",
    "#### How is it Different Than word2vec?\n",
    "\n",
    "- word2vec considers only the entire word, whereas `fasttext` will consider each suffix n-gram.\n",
    "\n",
    "As [Radim Hurek](https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html) says:\n",
    "\n",
    "> The main principle behind fastText is that the morphological structure of a word carries important information about the meaning of the word. Such structure is not taken into account by traditional word embeddings like Word2Vec, which train a unique word embedding for every individual word. This is especially significant for morphologically rich languages (German, Turkish) in which a single word can have a large number of morphological forms, each of which might occur rarely, thus making it hard to train good word embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "model = fasttext.train_unsupervised(\n",
    "    '../datasets/good_amazon_toy_reviews.txt', model='skipgram', lr=0.05, dim=100, ws=5, epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(model[\"adore\"].reshape(1,-1), model[\"love\"].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText Hyperparameters (From [Tutorial Notebook](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb))\n",
    "- **model**: Training architecture. Allowed values: `cbow`, `skipgram` (Default `cbow`)\n",
    "- **size**: Size of embeddings to be learnt (Default 100)\n",
    "- **alpha**: Initial learning rate (Default 0.025)\n",
    "- **window**: Context window size (Default 5)\n",
    "- **min_count**: Ignore words with number of occurrences below this (Default 5)\n",
    "- **loss**: Training objective. Allowed values: `ns`, `hs`, `softmax` (Default `ns`)\n",
    "- **sample**: Threshold for downsampling higher-frequency words (Default 0.001)\n",
    "- **negative**: Number of negative words to sample, for `ns` (Default 5)\n",
    "- **iter**: Number of epochs (Default 5)\n",
    "- **sorted_vocab**: Sort vocab by descending frequency (Default 1)\n",
    "- **threads**: Number of threads to use (Default 12)\n",
    "\n",
    "Hyperparameters unique to `fasttext`:\n",
    "- **min_n**: min length of char ngrams (Default 3)\n",
    "- **max_n**: max length of char ngrams (Default 6)\n",
    "- **bucket**: number of buckets used for hashing ngrams (Default 2000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Implementation of FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "text = list(pd.read_csv(\"../datasets/bbc-text.csv\")[\"text\"].values)\n",
    "\n",
    "new_text = [word_tokenize(story) for story in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a New FastText Model Using the Corpus Available\n",
    "\n",
    "You can check the parameters available for you to tune [here](https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText.train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7053763, 9176110)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FastText(vector_size=40, window=3, min_count=1)  # change the size of the windows\n",
    "model.build_vocab(new_text)\n",
    "model.train(new_text, total_examples=len(new_text), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get corpus total count\n",
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.2222363e-01, -2.2078693e+00,  3.5745579e-01,  6.5984428e-01,\n",
       "       -2.0824301e+00, -1.5883364e-01,  7.4407870e-01, -2.9683098e-01,\n",
       "       -1.4233255e-01,  2.2058125e-01, -3.3079988e-01, -2.2909251e-01,\n",
       "        6.1040205e-01,  9.1151112e-01, -1.4807616e+00, -1.0192211e+00,\n",
       "        1.2085850e+00,  4.1515335e-01, -1.4587786e+00,  6.5934646e-01,\n",
       "       -2.9038453e-01, -1.4285205e-01, -1.5967873e-01,  4.7649127e-01,\n",
       "       -9.2041689e-01,  8.4571254e-01,  8.7594099e-02,  7.1397549e-01,\n",
       "        1.3489476e+00, -1.1265091e+00,  3.8475925e-01, -5.4505509e-01,\n",
       "       -1.4048903e-01,  1.2123585e-04,  7.0672899e-01,  1.4613642e+00,\n",
       "       -6.0756701e-01,  5.5079877e-01,  1.3566753e+00,  2.4346066e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get word vector for dog\n",
    "model.wv[\"dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get length of word embeddings\n",
    "len(model.wv[\"king\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('francesca', 0.9700016975402832), ('francesco', 0.9565298557281494), ('francique', 0.9212310910224915), ('franchisee', 0.9076738357543945), ('franck', 0.9072319269180298), ('melancholia', 0.9044543504714966), ('northumberland', 0.9014002680778503), ('icelandic', 0.9005484580993652), ('francs', 0.9004991054534912), ('holland', 0.898938000202179)]\n",
      "\n",
      "\n",
      "[('56-man', 0.9893273115158081), ('22-man', 0.9890055656433105), ('44-man', 0.9877821803092957), ('pitman', 0.9865984320640564), ('25-man', 0.9860122799873352), ('13-man', 0.9850900173187256), ('15-man', 0.9841371178627014), ('kidman', 0.9826959371566772), ('30-man', 0.9824084639549255), ('ryman', 0.9820998311042786)]\n",
      "\n",
      "\n",
      "[('transcript', 0.9709033966064453), ('transcripts', 0.9535253643989563), ('transmit', 0.9499515295028687), ('transform', 0.948307991027832), ('transsexual', 0.9453391432762146), ('translate', 0.9408653974533081), ('traffic', 0.9380313158035278), ('tracy', 0.9353316426277161), ('transforms', 0.9351500868797302), ('transit', 0.9341503977775574)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"france\"))\n",
    "print(\"\\n\")\n",
    "print(model.wv.most_similar(\"aquaman\"))\n",
    "print(\"\\n\")\n",
    "print(model.wv.most_similar(\"transc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Memory Version of Paragraph Vector (PV-DM)\n",
    "![](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/doc2vec.png)\n",
    "\n",
    "### Distributed Bag of Words of Paragraph Vector (PV-DBOW)\n",
    "![](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/doc2vec2.png)\n",
    "[A Gentle Introduction to Doc2Vec](https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(reviews)]\n",
    "model = Doc2Vec(documents, vector_size=50, window=4, min_count=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_vector = model.infer_vector([\"The\", \"toy\", \"was\", \"broken\", \"quickly\"]).reshape(1, -1)\n",
    "doc2_vector = model.infer_vector([\"It\", \"broke\", \"fast\"]).reshape(1, -1)\n",
    "doc3_vector = model.infer_vector([\"I ate lunch late\"]).reshape(1,-1)\n",
    "doc4_vector = model.infer_vector([\"It\", \"was\", \"crappy\", \"quality\"]).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01620534]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(doc1_vector, doc2_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4734929]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(doc1_vector, doc3_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23254903]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(doc2_vector, doc3_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11511255]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(doc3_vector, doc4_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
