{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5KWA9b0FShn"
   },
   "source": [
    "# Problem Statement\n",
    "\n",
    "We want to understand what types of topics (subject matter) comprise the content in our documents. We can use **topic modelling** - using statistical methods to discovering the abstract/latent “topics” from a particular corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCHZ9R-SN1wU"
   },
   "source": [
    "## Load Data\n",
    "\n",
    "### BBC News Dataset\n",
    "\n",
    "We will load a BBC News dataset with documents divided between politics, entertainment, business, sport, and tech.\n",
    "\n",
    "### BBC Sport Dataset\n",
    "We will be loading in the BBC Sport news dataset. It is divided between 5 distinct sports topics - football, athletics, cricket, rugby and tennis.\n",
    "\n",
    "Both datasets are courtesy of *D. Greene and P. Cunningham. \"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\", Proc. ICML 2006.* ([Link](http://mlg.ucd.ie/datasets/bbc.html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcFTg-qVr8Zr"
   },
   "source": [
    "#### Define Function to Load in BBC Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YSRlvpiuhDs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "def load_bbc_corpus(directory: str, topics: List[str], num_docs: int)-> pd.DataFrame:\n",
    "    articles: List[Tuple[str, str]] = [(f\"../datasets/{directory}/{topic}/{str(i).zfill(3)}.txt\", topic) for i in range(1,num_docs + 1) for topic in TOPICS]\n",
    "    data = []\n",
    "    for article, topic in articles:\n",
    "        with open(article, encoding=\"latin1\") as article: # open each sports article\n",
    "            content = article.read()\n",
    "            data.append({\"topic\": topic, \"text\": content})\n",
    "\n",
    "    # generate a dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "    df.text = df.text.apply(lambda text: text.replace(\"\\n\", \" \"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5ub2vp_vHPy"
   },
   "source": [
    "#### Load BBC Sport Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "RyKLJWxkq8qK",
    "outputId": "a15d71d3-3441-4b78-8515-c696df5b42b6"
   },
   "outputs": [],
   "source": [
    "TOPICS = [\"football\", \"athletics\", \"cricket\", \"rugby\", \"tennis\"]\n",
    "sports_corpus_df = load_bbc_corpus(\"bbcsport\", TOPICS, num_docs=100)\n",
    "sports_corpus_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uj-PPwSMr5XB"
   },
   "source": [
    "#### Load in BBC News Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "AC6ej7KAq-Sb",
    "outputId": "98f2c884-52e9-4a2e-bae1-097cb73f31a4"
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "TOPICS = [\"business\", \"sport\", \"entertainment\", \"tech\", \"politics\"]\n",
    "news_corpus_df = load_bbc_corpus(\"bbc\", TOPICS, num_docs=350)\n",
    "news_corpus_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZ6dD24qIq_3"
   },
   "source": [
    "### Technique 1: Non-Negative Matrix Factorization\n",
    "\n",
    "We can think of our corpus as a two-dimensional table - rows being the documents, and columns being the features (ie. in a count-based vectorizer, each column being a unique token).\n",
    "\n",
    "In Non-Negative Matrix Factorization, we try to find two matrices `W` and `H`, that contain only nonnegative values and when multiplied together, will reconstruct `X`. \n",
    "\n",
    "We need to select a variable `K`, which is the number of components/topics we wish to use.\n",
    "\n",
    "If we want to model topics for a $N \\times M$ matrix `X`, where each value is non-negative, then NMD will produce a $K \\times M$ matrix `H` and a $N \\times K$ matrix `W`.\n",
    "![NMF](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/nmf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0k3VlhGe4Eq"
   },
   "source": [
    "#### Step 1: Vectorize The Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "oWL3hhZoFQkY",
    "outputId": "2bd3f583-2d10-40f2-f50a-917e1472d4be"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2),\n",
    "                             min_df=0.01, max_df=0.4, stop_words=\"english\")\n",
    "\n",
    "news_vectorizer = TfidfVectorizer(ngram_range=(3,3), min_df=3,\n",
    "                            max_df=0.4, stop_words=\"english\")\n",
    "\n",
    "X_sport, sports_terms = vectorizer.fit_transform(sports_corpus_df.text), vectorizer.get_feature_names_out()\n",
    "X_news, news_terms = news_vectorizer.fit_transform(news_corpus_df.text), news_vectorizer.get_feature_names_out()\n",
    "sport_tf_idf = pd.DataFrame(X_sport.toarray(), columns=sports_terms)\n",
    "news_tf_idf = pd.DataFrame(X_news.toarray(), columns=news_terms)\n",
    "print(f\"News TF-IDF: {news_tf_idf.shape}\")\n",
    "print(news_tf_idf.head(5))\n",
    "print(f\"Sports TF-IDF: {sport_tf_idf.shape}\")\n",
    "sport_tf_idf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98EorTt9e9VY"
   },
   "source": [
    "#### Step 2: Fit NMF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20H1upyZOQ7F",
    "outputId": "793702a1-fc2d-4f52-e0e6-c944b06d70e1"
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=5)\n",
    "W_sport = nmf.fit_transform(X_sport)\n",
    "H_sport = nmf.components_\n",
    "print(f\"Original shape of X sports is {X_sport.shape}\")\n",
    "print(f\"Decomposed W sports matrix is {W_sport.shape}\")\n",
    "print(f\"Decomposed H sports matrix is {H_sport.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3qf86a1tBsw",
    "outputId": "16000a61-b8c4-4ea2-ee8f-3d5075bdf0b5"
   },
   "outputs": [],
   "source": [
    "W_news = nmf.fit_transform(X_news)\n",
    "H_news = nmf.components_\n",
    "print(f\"Original shape of X news is {X_news.shape}\")\n",
    "print(f\"Decomposed W news matrix is {W_news.shape}\")\n",
    "print(f\"Decomposed H news matrix is {H_news.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WKR9Xbdes_aj",
    "outputId": "edcb6b77-fb9d-4b43-aa06-354df5340652"
   },
   "outputs": [],
   "source": [
    "W_sport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2N8OPYzStQT7",
    "outputId": "2d99fdbb-3f06-46e9-8228-b867a410ea86"
   },
   "outputs": [],
   "source": [
    "H_sport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TB5g5J1TfJYG"
   },
   "source": [
    "#### Step 3: Report Results For Each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "unctGPrXPLp7",
    "outputId": "f3314cec-7dde-4651-bc7d-d6009311eebb"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "def get_top_tf_idf_tokens_for_topic(H: np.array, feature_names: List[str], num_top_tokens: int = 5):\n",
    "  \"\"\"\n",
    "  Uses the H matrix (K components x M original features) to identify for each\n",
    "  topic the most frequent tokens.\n",
    "  \"\"\"\n",
    "  for topic, vector in enumerate(H):\n",
    "    print(f\"TOPIC {topic}\\n\")\n",
    "    total = vector.sum()\n",
    "    top_scores = vector.argsort()[::-1][:num_top_tokens]\n",
    "    token_names = list(map(lambda idx: feature_names[idx], top_scores))\n",
    "    strengths = list(map(lambda idx: vector[idx] / total, top_scores))\n",
    "    \n",
    "    for strength, token_name in zip(strengths, token_names):\n",
    "      print(f\"\\b{token_name} ({round(strength * 100, 1)}%)\\n\")\n",
    "    print(f\"=\" * 50)\n",
    "\n",
    "get_top_tf_idf_tokens_for_topic(H_sport, sport_tf_idf.columns.tolist(), 5)\n",
    "print(f\"BBC News Topics:\\n\\n\")\n",
    "get_top_tf_idf_tokens_for_topic(H_news, news_tf_idf.columns.tolist(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_ooShHVfjzl"
   },
   "source": [
    "#### Get the Top Documents For Each Topic\n",
    "\n",
    "We can also use the `W` matrix to grab top documents per topic (ie. the document that had the greatest percentage of of each topic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6THdZxjYP5Kn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_top_documents_for_each_topic(W: np.array, documents: List[str], num_docs: int = 5):\n",
    "    sorted_docs = W.argsort(axis=0)[::-1]\n",
    "    top_docs = sorted_docs[:num_docs].T\n",
    "    per_document_totals = W.sum(axis=1)\n",
    "    for topic, top_documents_for_topic in enumerate(top_docs):\n",
    "        print(f\"Topic {topic}\")\n",
    "        for doc in top_documents_for_topic:\n",
    "            score = W[doc][topic]\n",
    "            percent_about_topic = round(score / per_document_totals[doc] * 100, 1)\n",
    "            print(f\"{percent_about_topic}%\", documents[doc])\n",
    "            print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vEfsy0IIGDm"
   },
   "outputs": [],
   "source": [
    "get_top_documents_for_each_topic(W_sport, sports_corpus_df.text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pIFXsYHHuQmw",
    "outputId": "dec15820-74b3-4724-b972-d503aeedc7f4"
   },
   "outputs": [],
   "source": [
    "get_top_documents_for_each_topic(W_news, news_corpus_df.text.tolist(), num_docs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNr4nJK9xPi0"
   },
   "source": [
    "### Approach 2: LSA (Latent Semantic Analysis)\n",
    "\n",
    "We can also leverage a dimensionality reduction technique that we've ecnountered before - **Singular Value Decomposition (SVD)** to perform topic modelling.\n",
    "\n",
    "The following diagram and code snippet is from *Blueprints for Text Analytics Using Python*, Albrecht et al.\n",
    "\n",
    "Remember that for SVD, we can take our original matrix and decompose it into three matrices.\n",
    "\n",
    "$$\n",
    "V = U \\times \\Sigma \\times V^{\\star}\n",
    "$$\n",
    "We can use these three decomposed matrices for different purposes ([Adel Amadyan](https://adel.ac/singular-value-decomposition-in-computer-vision/#Singular_Value_Decomposition182)):\n",
    "![SVD Topic Modelling](https://i1.wp.com/adel.ac/wp-content/uploads/2019/02/svd.png?resize=500%2C200) \n",
    "\n",
    "The $U$ matrix will provide a signal for what the topic composition of our documents are.\n",
    "\n",
    "The diagonal elements of the $\\Sigma$ matrix can be used to estimate the \"strength\" of each topic. \n",
    "\n",
    "The $V^{star}$ matrix can be used to find the top associated words with each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJ1cLeoh7eCG"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# we need to select a K (the number of topics)\n",
    "K = 5\n",
    "\n",
    "svd = TruncatedSVD(n_components=K)\n",
    "U = svd.fit_transform(X_news)\n",
    "V_star = svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ucR-E1abHr9u",
    "outputId": "93279534-2f5a-4c77-8eb2-0fd0611aaa11"
   },
   "outputs": [],
   "source": [
    "print(f\"U shape is {U.shape}\")\n",
    "get_top_documents_for_each_topic(U, news_corpus_df.text.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NijxrxtJkuG"
   },
   "source": [
    "### Approach 3: Latent Dirichlet Allocation\n",
    "\n",
    "The final approach uses a probablistic sampling method by viewing each document as consisting of mixture of different topics, which are themselves mixtures of different words. \n",
    "\n",
    "Each of the mixtures (documents as mixtures of topics, topics as mixtures of words) are modelled using a [Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution).\n",
    "\n",
    "The algorithm creates initial Dirichlet distributions from each topic and word and tries to recreate the original words used for a document using sampling.\n",
    "\n",
    "It first attempts to construct the representative words for a topic ([A Beginner's Guide to Latent Dirichlet Allocation, Ria Kulshrestha](https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2)):\n",
    "![https://miro.medium.com/max/1222/1*NjeMT281GMduRYvPIS8IjQ.png](https://miro.medium.com/max/1222/1*NjeMT281GMduRYvPIS8IjQ.png)\n",
    "\n",
    "The algorithm looks like this:\n",
    "\n",
    "![LDA](https://miro.medium.com/max/494/1*VTHd8nB_PBsDtd2hd87ybg.png) \n",
    "\n",
    "(from [LDA Topic Modeling: An Explanation](https://towardsdatascience.com/lda-topic-modeling-an-explanation-e184c90aadcd))\n",
    "\n",
    "* $\\alpha$ is the per-document topic distributions\n",
    "* $\\phi$ is the word distribution for a given topic\n",
    "* $\\beta$ is the per-topic word distributions\n",
    "* $\\theta$ is the topic distribution for the m-th document\n",
    "* $Z$ is the topic assigned to the n-th word of the m-th document\n",
    "\n",
    "We can only observe $W$. β is the table in the above screenshot (each row is a topic, each column is a word). \n",
    "\n",
    "We randomly initialize the initial topic distribution, and update iteratively until we converge to a solution or exceed the maximum number of iterations.\n",
    "\n",
    "The New York Times [highlighed an example of a recommendation system based off of LDA](https://open.blogs.nytimes.com/2015/08/11/building-the-next-new-york-times-recommendation-engine/).\n",
    "\n",
    "#### Assumptions of LDA\n",
    "* Bag of words - each document is a bag of words where sequence, part of speech, etc. are not considered. \n",
    "* The number of topics is pre-determined and known (or guesstimated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V14OueMHHv6_",
    "outputId": "2df076b4-56a0-4d7e-d080-12667df3214b"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5)\n",
    "W = lda.fit_transform(X_sport)\n",
    "get_top_documents_for_each_topic(W, sports_corpus_df.text.tolist(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxX-eWtK44-L"
   },
   "source": [
    "### Collaborative Filters\n",
    "\n",
    "We can also leverage NLP models as part of a collaborative filter in order to generate product recommendations for a given user/customer.\n",
    "\n",
    "A common approach is constructing an **User-Item** Iteraction Matrix, where there are many sparse elements. We can then iteratively fill compute the User and Item matrices that will minimize the least square error. This class of algorithms is called **Alternating Least Square**.\n",
    "![https://miro.medium.com/max/1400/1*xMxQL_V9CWeLggrk-Uyzmg.png](https://miro.medium.com/max/1400/1*xMxQL_V9CWeLggrk-Uyzmg.png).\n",
    "\n",
    "See here for an [example of using collaborative filters in Spark](https://gist.github.com/ychennay/0dc72d5098aa209985feed1a6b4f6b96).\n",
    "\n",
    "[Himanshu Kriplani, \"Alternating Least Square for Implicit Dataset with code\"](https://towardsdatascience.com/alternating-least-square-for-implicit-dataset-with-code-8e7999277f4b). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKfrsfnsxJb-"
   },
   "source": [
    "#### Appendix: Mathematical Theory for Non-Negative Matrix Factorization\n",
    "\n",
    "Derivations are from [Source Separation Tutorial Mini-Series II: Introduction to Non-Negative Matrix Factorization](https://ccrma.stanford.edu/~njb/teaching/sstutorial/part2.pdf).\n",
    "\n",
    "We attempt to minimize the divergence $D$, between the original matrix $X$ and the product of the deconstructed $W$ and $H$ matrices:\n",
    "$$\n",
    "min(D(V||\\hat{V}))\n",
    "$$\n",
    "For NMF, this means\n",
    "$$\n",
    "min_{W, H >= 0}(D(V||W\\times H))\n",
    "$$\n",
    "This is read as *we want to select non-negative values for $W$ and $H$ that will minimize $D$*.\n",
    "\n",
    "There are many functions we can choose to approximate $D$ for examle, **Euclidean Distance**:\n",
    "$$\n",
    "D(V||\\hat{V}) = \\sum_{i,j}{(V_{ij} - \\hat{V}_{ij})^{2}}\n",
    "$$\n",
    "However, in practice, we commonly select **[Kullback-Leibler Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)** to use for the divergence metric:      \n",
    "$$\n",
    "D(V||\\hat{V}) = \\sum_{i,j}{(V_{ij}log(\\frac{V_{ij}}{\\hat{V}_{ij}}) - V_{ij} + \\hat{V}_{ij})}\n",
    "$$\n",
    "We can rewrite this as (by substituting $V_{ij}$ with $W\\times H$):\n",
    "$$\n",
    "D(V||\\hat{V}) = \\sum_{i,j}{(V_{ij}log(\\frac{V_{ij}}{W\\times H}) - V_{ij} + W\\times H)}\n",
    "$$\n",
    "\n",
    "From here, we can use [Jensen's Inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality) to rewrite this as \n",
    "$$\n",
    "H^{\\star}_{kj} = \\frac{\\sum{V_{ij}\\pi_{ijk}}}{\\sum{W_{ik}}}\n",
    "$$\n",
    "Here, $\\pi_{ijk}$ is how much of the component $k$ to assign to the i-th document's j-th feature.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Topic Modelling",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
