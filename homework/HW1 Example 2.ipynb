{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b185174",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HW1\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# input text\n",
    "review_good = open('../datasets/good_amazon_toy_reviews.txt', 'r', encoding = 'utf-8')\n",
    "review_g = review_good.readlines()\n",
    "\n",
    "review_poor = open('../datasets/poor_amazon_toy_reviews.txt', 'r', encoding = 'utf-8')\n",
    "review_p = review_poor.readlines()\n",
    "\n",
    "# combine good reviews and poor reviews, remove \\n\n",
    "reviews = review_g + review_p\n",
    "for i in range(len(reviews)):\n",
    "    reviews[i] = reviews[i].replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c9c2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114917"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show total number of reviews\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e6ec37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Excellent!!!',\n",
       " '\"Great quality wooden track (better than some others we have tried). Perfect match to the various vintages of Thomas track that we already have. There is enough track here to have fun and get creative incorporating your key pieces with track splits, loops and bends.\"',\n",
       " 'my daughter loved it and i liked the price and it came to me rather than shopping with a ton of people around me. Amazon is the Best way to shop!',\n",
       " 'Great item. Pictures pop thru and add detail as &#34;painted.&#34;  Pictures dry and it can be repainted.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show first four reviews\n",
    "reviews[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6f541ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART A\n",
    "# remove incorrect decodings\n",
    "for i in range(len(reviews)):\n",
    "    reviews[i] = re.sub(r'<\\w+\\s?\\/?>', '',reviews[i]) # remove words like <br /> or <br>\n",
    "    reviews[i] = re.sub(r'&#\\w{2,};', '',reviews[i]) # remove words like &#34; patterns according to this entities list https://www.freeformatter.com/html-entities.html\n",
    "    reviews[i] = re.sub(r'%\\w{2}', '',reviews[i]) # remove words like %20; patterns according to encoding reference https://krypted.com/utilities/html-encoding-reference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c522933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great item. Pictures pop thru and add detail as painted.  Pictures dry and it can be repainted.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the fourth review that has incorrect decoding before\n",
    "reviews[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7110f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize all reference\n",
    "\n",
    "# create list of possible patterns for each category, process words in upper case later\n",
    "\n",
    "# recipients - children: possible patterns: 2-year/years/yr/yrs-old, seven year/years/yr/yrs old, 2 yr. old, (grand)son(s)/daughter(s)/boy(s)/girl(s)/kid(s)/child(ren)/baby/babies/sister(s)/brother(s)/cousin(s)/nephew(s)/niece(s)\n",
    "# children = [r'\\b(\\w+\\s?\\-?(?:years?|yrs?)\\.?\\-?\\s?old)\\b', r'\\b(child(?:ren)?|kids?|boys?|girls?|sons?|daughters?|sisters?|brothers?|cousins?|nephews?|nieces?|bab(?:y|ies)|wild)\\b', r'\\b(grand(?:child(?:ren)?|kids?|sons?|daughters?))\\b']\n",
    "children = [r'\\b(\\w+\\s?\\-?(?:years?|yrs?)\\.?\\-?\\s?old)\\b', r'\\b((?:grand)?(?:child(?:ren)?|kids?|boys?|girls?|sons?|daughters?|sisters?|brothers?|cousins?|nephews?|nieces?|bab(?:y|e|ies)|wild))\\b']\n",
    "\n",
    "# recipients - parents: father, mother, grandfather, grandmother, parents, grandparents, aunt, uncle\n",
    "# parents = [r'\\b(father|mother|parents?|aunts?|uncles?|dad(?:dy)?|mom(?:my)?|papa|mama)\\b', r'\\b(grand(?:father|mother|parents?|pa|ma))\\b']\n",
    "parents = [r'\\b((?:grand)?(?:father|mother|parents?|aunts?|uncles?|dad(?:dy)?|mom(?:my)?|pa(?:pa)?|ma(?:ma)?))\\b']\n",
    "\n",
    "# recipients - spouses and other relationship: wife, husband, darling, my love/honey\n",
    "spouses = [r'\\b(hu(?:sband|bby)|wi(?:fe|ves)|darlings?|(?:boy|girl)friend|partners?)\\b', r'\\b(my\\s(?:love|honey))\\b']\n",
    "\n",
    "# combine all patterns of recipients\n",
    "recipients = children + parents + spouses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ead9bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# occasions - Chistmas, Anniversary, Birthday, Ceremony, Festival, Halloween, etc.\n",
    "occasions = [r'\\b((?:christ|x)mas|anniversar(?:y|ies)|b(?:irth)?day|ceremon(?:y|ies)|festivals?|halloween)\\b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c20ac6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new reviews list with all reviews in lower case\n",
    "reviews_low = []\n",
    "\n",
    "# apply all the patterns to each review\n",
    "for i in range(len(reviews)):\n",
    "    new_review = reviews[i].lower()\n",
    "    # replace all relevant words with _RECIPIENT_\n",
    "    for j in recipients:\n",
    "        new_review = re.sub(j, '_RECIPIENT_', new_review)\n",
    "    # replace all relevant words with _GIFT_OCCASION_\n",
    "    for k in occasions:\n",
    "        new_review = re.sub(k, '_GIFT_OCCASION_', new_review) \n",
    "    reviews_low.append(new_review)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e7140fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['excellent!!!',\n",
       " '\"great quality wooden track (better than some others we have tried). perfect match to the various vintages of thomas track that we already have. there is enough track here to have fun and get creative incorporating your key pieces with track splits, loops and bends.\"',\n",
       " 'my _RECIPIENT_ loved it and i liked the price and it came to me rather than shopping with a ton of people around me. amazon is the best way to shop!',\n",
       " 'great item. pictures pop thru and add detail as painted.  pictures dry and it can be repainted.',\n",
       " 'i was pleased with the product.',\n",
       " '_RECIPIENT_ like it']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_low[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52171c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART B\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52704a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to the local memory processing problem for the large scale text, I will sample for 20000 documents/rows for vectorization 1-5\n",
    "import random\n",
    "random.seed(0)\n",
    "sampled_reviews = random.sample(reviews_low, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d8fa850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe is (20000, 17481)\n"
     ]
    }
   ],
   "source": [
    "# 1. TFIDF vectorizer, no stopword removal or other preprocessing\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(sampled_reviews)\n",
    "vector_1 = vectorizer.transform(sampled_reviews)\n",
    "review_df = pd.DataFrame(vector_1.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(f\"Shape of dataframe is {review_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "870b4dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe is (20000, 17481)\n"
     ]
    }
   ],
   "source": [
    "# 2. Count vectorizer, no stopword removal or other preprocessing\n",
    "\n",
    "vectorizer = CountVectorizer() \n",
    "vectorizer.fit(sampled_reviews)\n",
    "vector_2 = vectorizer.transform(sampled_reviews)\n",
    "review_df = pd.DataFrame(vector_2.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(f\"Shape of dataframe is {review_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c40aadeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe is (20000, 17188)\n"
     ]
    }
   ],
   "source": [
    "# 3. either vectorizer (Count vectorizer) + stop words removed\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english') \n",
    "vectorizer.fit(sampled_reviews)\n",
    "vector_3 = vectorizer.transform(sampled_reviews)\n",
    "review_df = pd.DataFrame(vector_3.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(f\"Shape of dataframe is {review_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a05ac31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe is (20000, 12515)\n"
     ]
    }
   ],
   "source": [
    "# 4. either vectorizer (Count vectorizer) + stop words removed + stemming\n",
    "\n",
    "# create list for stemmed reviews\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# function to stem each sentence in list\n",
    "def stem_list(list):\n",
    "    stemmed_reviews = []\n",
    "    for review in list:\n",
    "        words = word_tokenize(review)\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_words.append(stemmer.stem(word))\n",
    "        stemmed_review = \" \".join(new_words)\n",
    "        stemmed_reviews.append(stemmed_review)\n",
    "    return stemmed_reviews\n",
    "\n",
    "stemmed_reviews = stem_list(sampled_reviews)\n",
    "\n",
    "# vectorization with stemming\n",
    "vectorizer = CountVectorizer(stop_words='english') \n",
    "vectorizer.fit(stem_list(stemmed_reviews))\n",
    "vector_4 = vectorizer.transform(stem_list(stemmed_reviews))\n",
    "review_df = pd.DataFrame(vector_4.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(f\"Shape of dataframe is {review_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39e30817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe is (20000, 14297)\n"
     ]
    }
   ],
   "source": [
    "# 5. either vectorizer (Count vectorizer) + stop words removed + lemmatization\n",
    "\n",
    "# create list for lemmatized reviews\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "## refer to https://gist.github.com/gaurav5430/9fce93759eb2f6b1697883c3782f30de#file-nltk-lemmatize-sentences-py\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "# function to lemmatize each sentence in list\n",
    "def lemma_list(list):\n",
    "    lemma_reviews = []\n",
    "    for review in list:\n",
    "        lemma_review = lemmatize_sentence(review)\n",
    "        lemma_reviews.append(lemma_review)\n",
    "    return lemma_reviews\n",
    "\n",
    "lemma_reviews = lemma_list(sampled_reviews)\n",
    "\n",
    "# vectorization with lemmatization\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer.fit(lemma_reviews)\n",
    "vector_5 = vectorizer.transform(lemma_reviews)\n",
    "review_df = pd.DataFrame(vector_5.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(f\"Shape of dataframe is {review_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c3406b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe with ngram size = 2 is (114917, 10000)\n"
     ]
    }
   ],
   "source": [
    "# 6. either vectorizer (Count vectorizer) + stop words removed + lemmatization + ngram size of 2\n",
    "# due to the memory problem, the 20000 sampled reviews list doesn't work, so here I use unsampled data with feature limit 10000\n",
    "\n",
    "# original reviews with lemmatization\n",
    "lemma_reviews_full = lemma_list(reviews_low)\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range = (2,2), max_features = 10000) \n",
    "vectorizer.fit(lemma_reviews_full)\n",
    "vector_6 = vectorizer.transform(lemma_reviews_full)\n",
    "review_df = pd.DataFrame(vector_6.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(f\"Shape of dataframe with ngram size = 2 is {review_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e6db491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe with ngram size = 3 is (114917, 10000)\n"
     ]
    }
   ],
   "source": [
    "# 7. either vectorizer (Count vectorizer) + stop words removed + lemmatization + ngram size of 3\n",
    "# due to the memory problem, the 20000 sampled reviews list doesn't work, so here I use unsampled data with feature limit\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range = (3,3), max_features = 10000) \n",
    "vectorizer.fit(lemma_reviews_full)\n",
    "vector_7 = vectorizer.transform(lemma_reviews_full)\n",
    "review_df = pd.DataFrame(vector_7.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(f\"Shape of dataframe with ngram size = 3 is {review_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed5e7ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of csr for vectorization 1 is (114917, 40514)\n",
      "Shape of csr for vectorization 2 is (114917, 40514)\n",
      "Shape of csr for vectorization 3 is (114917, 40210)\n",
      "Shape of csr for vectorization 4 is (114917, 30844)\n",
      "Shape of csr for vectorization 5 is (114917, 35211)\n",
      "Shape of csr for vectorization 6 is (114917, 561625)\n",
      "Shape of csr for vectorization 7 is (114917, 994608)\n"
     ]
    }
   ],
   "source": [
    "## appendix: all csr shape of above vectorizations with original data (without sampling or feature limit)\n",
    "\n",
    "vectors = []\n",
    "#1 TFIDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(reviews_low)\n",
    "vectors.append(vectorizer.transform(reviews_low))\n",
    "\n",
    "#2 count vectorizer\n",
    "vectorizer = CountVectorizer() \n",
    "vectorizer.fit(reviews_low)\n",
    "vectors.append(vectorizer.transform(reviews_low))\n",
    "\n",
    "#3 count vectorizer + stopword\n",
    "vectorizer = CountVectorizer(stop_words='english') \n",
    "vectorizer.fit(reviews_low)\n",
    "vectors.append(vectorizer.transform(reviews_low))\n",
    "\n",
    "#4 count vectorizer + stopword + stemming\n",
    "vectorizer = CountVectorizer(stop_words='english') \n",
    "vectorizer.fit(stem_list(reviews_low))\n",
    "vectors.append(vectorizer.transform(stem_list(reviews_low)))\n",
    "\n",
    "#5 count vectorizer + stopword + lemmatization\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer.fit(lemma_reviews_full)\n",
    "vectors.append(vectorizer.transform(lemma_reviews_full))\n",
    "\n",
    "#6 count vectorizer+ stopword + lemmatization + ngram size of 2\n",
    "vectorizer = CountVectorizer(stop_words='english', ngram_range = (2,2)) \n",
    "vectorizer.fit(lemma_reviews_full)\n",
    "vectors.append(vectorizer.transform(lemma_reviews_full))\n",
    "\n",
    "#7 stopword + lemmatization + ngram size of 3\n",
    "vectorizer = CountVectorizer(stop_words='english', ngram_range = (3,3)) \n",
    "vectorizer.fit(lemma_reviews_full)\n",
    "vectors.append(vectorizer.transform(lemma_reviews_full))\n",
    "\n",
    "for i in range(len(vectors)):\n",
    "    print(f\"Shape of csr for vectorization {i+1} is {vectors[i].shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d4a20816497f3ee1b8eabca57e1499f026e688c57304d4c2e02f4791e91a7c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
